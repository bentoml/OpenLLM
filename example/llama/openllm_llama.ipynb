{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKoT8vTPG1LrM0UcdcggOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bentoml/OpenLLM/blob/feat%2Fllama-example/example/llama/openllm_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz2x8naaLWzN"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "!pip install openllm[llama] bentoml vllm --upgrade >/dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [optional] Check the memory, and gpu info you have\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || ture\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    #print(\"RUN `openllm models` to find modles which can runable on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei23zHY-NHhh",
        "outputId": "592420be-700f-414b-cce8-d346c8f8fe2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal: 12.68 GB\n",
            "=============GPU INFO=============\n",
            "GPU NOT available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the llama service, modify this if you want to customize\n",
        "%%file service.py\n",
        "import bentoml\n",
        "import openllm\n",
        "import openllm_core\n",
        "import os\n",
        "import typing as t\n",
        "\n",
        "#run `openllm models` to find more model IDs of llama2\n",
        "MODEL_ID = \"NousResearch/llama-2-7b-chat-hf\"  #@param [\"NousResearch/llama-2-7b-chat-hf\", \"NousResearch/llama-2-13b-chat-hf\",\"NousResearch/llama-2-70b-chat-hf\"]\n",
        "BACKEND = \"pt\"  #@param [\"pt\", \"vllm\"]\n",
        "\n",
        "\n",
        "os.environ['OPENLLM_MODEL_ID'] = MODEL_ID\n",
        "os.environ['OPENLLM_BACKEND'] = BACKEND\n",
        "\n",
        "model = \"llama\"\n",
        "\n",
        "llm_config = openllm.AutoConfig.for_model(model)\n",
        "llm_runner = openllm.Runner(model, llm_config=llm_config)\n",
        "\n",
        "svc = bentoml.Service(name=\"llama-service\", runners=[llm_runner])\n",
        "\n",
        "_JsonInput = bentoml.io.JSON.from_sample({'prompt': '', 'llm_config': llm_config.model_dump(flatten=True), 'adapter_name': None})\n",
        "\n",
        "\n",
        "@svc.on_startup\n",
        "def download(_: bentoml.Context):\n",
        "  llm_runner.download_model()\n",
        "\n",
        "@svc.api(route='/v1/generate', input=_JsonInput, output=bentoml.io.JSON.from_sample({'responses': [], 'configuration': llm_config.model_dump(flatten=True)}))\n",
        "async def generate_v1(input_dict: dict[str, t.Any]) -> openllm.GenerationOutput:\n",
        "  qa_inputs = openllm.GenerationInput.from_llm_config(llm_config)(**input_dict)\n",
        "  config = qa_inputs.llm_config.model_dump()\n",
        "  if llm_runner.backend == 'vllm':\n",
        "    responses = await llm_runner.vllm_generate.async_run(qa_inputs.prompt, adapter_name=qa_inputs.adapter_name, request_id=openllm_core.utils.gen_random_uuid(), **config)\n",
        "  else:\n",
        "    responses = await llm_runner.generate.async_run(qa_inputs.prompt, adapter_name=qa_inputs.adapter_name, **config)\n",
        "  return openllm.GenerationOutput(responses=responses, configuration=config)\n",
        "\n",
        "@svc.api(route='/v1/generate_stream', input=_JsonInput, output=bentoml.io.Text(content_type='text/event-stream'))\n",
        "async def generate_stream_v1(input_dict: dict[str, t.Any]) -> t.AsyncGenerator[str, None]:\n",
        "  echo = input_dict.pop('echo', False)\n",
        "  qa_inputs = openllm.GenerationInput.from_llm_config(llm_config)(**input_dict)\n",
        "  if llm_runner.backend == 'vllm':\n",
        "    return llm_runner.vllm_generate_iterator.async_stream(qa_inputs.prompt,\n",
        "                                                      adapter_name=qa_inputs.adapter_name,\n",
        "                                                      echo=echo,\n",
        "                                                      request_id=openllm_core.utils.gen_random_uuid(),\n",
        "                                                      **qa_inputs.llm_config.model_dump())\n",
        "  else:\n",
        "    return llm_runner.generate_iterator.async_stream(qa_inputs.prompt, adapter_name=qa_inputs.adapter_name, echo=echo, **qa_inputs.llm_config.model_dump())\n"
      ],
      "metadata": {
        "id": "aIoVxUrUOBGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412a5a77-5c26-42ec-e7db-a3ec93dce88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the bentofile.yaml (modify it following https://docs.bentoml.com/en/latest/concepts/bento.html)\n",
        "%%file bentofile.yaml\n",
        "service: 'service:svc'\n",
        "include:\n",
        "  - '*.py'\n",
        "python:\n",
        "  packages:\n",
        "    - openllm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCREWwJ2Btow",
        "outputId": "a3b76434-53bb-4e1b-f6bc-8036d869fc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bentofile.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build the llama bento using bentoml (service.py bentofile.yaml)\n",
        "!bentoml build -f bentofile.yaml"
      ],
      "metadata": {
        "id": "lAdxFN6wXpHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Or build bentos using Openllm\n",
        "#RUN `openllm build -h` for help\n",
        "!openllm build llama --model-id NousResearch/llama-2-7b-chat-hf --backend pt"
      ],
      "metadata": {
        "id": "aIUo9lvstyQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check the bentos you just build, and push them to bentocloud if you want\n",
        "! bentoml list\n",
        "endpoint = input(\"input endpoint (like https://xxx.cloud.bentoml.com): \")\n",
        "token = input(\"input token (please follow https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html#creating-an-api-token):\")\n",
        "\n",
        "#!bentoml cloud login --api-token {token} --endpoint {endpoint} --context colab-user\n",
        "!bentoml push xxxx    --context colab-user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuWN1pksDu_U",
        "outputId": "89981cf2-9257-4462-cb60-318cbde42c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m \u001b[0m\u001b[1mTag                                           \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mSize     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCreation Time      \u001b[0m\u001b[1m \u001b[0m\n",
            " llama-service:lqwm2tctp2d3masc                  16.42 KiB  2023-09-15 04:17:08 \n",
            " nousresearch--llama-2-7b-hf-service:dacdfcde3â€¦  34.43 KiB  2023-09-15 03:39:54 \n",
            "[\n",
            "  \"colab-user\"\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [optional] Start the llama server locally using `bentoml` command\n",
        "from google.colab.output import eval_js\n",
        "print(\"try it out in %s\" % eval_js(\"google.colab.kernel.proxyPort(3000)\"))\n",
        "!bentoml serve service:svc -q"
      ],
      "metadata": {
        "id": "dpoLNTlUPHuG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "617f7063-f053-4055-dade-818bb662789a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "try it out in https://dssd5iaijrq-496ff2e9c6d22116-3000-colab.googleusercontent.com/\n"
          ]
        }
      ]
    }
  ]
}