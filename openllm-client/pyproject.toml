[build-system]
build-backend = "hatchling.build"
requires = [
  "hatchling==1.18.0",
  "hatch-vcs==0.3.0",
  "hatch-fancy-pypi-readme==23.1.0",
]

[project]
authors = [
  { name = "Aaron Pham", email = "aarnphm@bentoml.com" },
  { name = "BentoML Team", email = "contact@bentoml.com" },
]
dynamic = ['readme', 'version']
classifiers = [
  "Development Status :: 5 - Production/Stable",
  "Environment :: GPU :: NVIDIA CUDA",
  "Environment :: GPU :: NVIDIA CUDA :: 12",
  "Environment :: GPU :: NVIDIA CUDA :: 11.8",
  "Environment :: GPU :: NVIDIA CUDA :: 11.7",
  "License :: OSI Approved :: Apache Software License",
  "Topic :: Scientific/Engineering",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries",
  "Operating System :: OS Independent",
  "Intended Audience :: Developers",
  "Intended Audience :: Science/Research",
  "Intended Audience :: System Administrators",
  "Typing :: Typed",
  "Programming Language :: Python",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3 :: Only",
  "Programming Language :: Python :: 3.8",
  "Programming Language :: Python :: 3.9",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: Implementation :: CPython",
  "Programming Language :: Python :: Implementation :: PyPy",
]
description = "OpenLLM Client: Interacting with OpenLLM HTTP/gRPC server, or any BentoML server."
keywords = [
  "MLOps",
  "AI",
  "BentoML",
  "Model Serving",
  "Model Deployment",
  "LLMOps",
  "Falcon",
  "Vicuna",
  "Llama 2",
  "Fine tuning",
  "Serverless",
  "Large Language Model",
  "Generative AI",
  "StableLM",
  "Alpaca",
  "PyTorch",
  "Transformers",
]
dependencies = ["openllm-core", "httpx"]
license = "Apache-2.0"
name = "openllm-client"
requires-python = ">=3.8"
[project.urls]
Blog = "https://modelserving.com"
Chat = "https://discord.gg/openllm"
Documentation = "https://github.com/bentoml/OpenLLM/blob/main/openllm-client/README.md"
GitHub = "https://github.com/bentoml/OpenLLM/blob/main/openllm-client"
History = "https://github.com/bentoml/OpenLLM/blob/main/CHANGELOG.md"
Homepage = "https://bentoml.com"
Tracker = "https://github.com/bentoml/OpenLLM/issues"
Twitter = "https://twitter.com/bentomlai"
[project.optional-dependencies]
full = ["openllm-client[grpc,agents]"]
grpc = ["bentoml[grpc]>=1.0.25"]
agents = ["transformers[agents]>=4.30", "diffusers", "soundfile"]

[tool.hatch.version]
fallback-version = "0.0.0"
source = "vcs"
[tool.hatch.build.hooks.vcs]
version-file = "src/openllm_client/_version.py"
[tool.hatch.version.raw-options]
git_describe_command = [
  "git",
  "describe",
  "--dirty",
  "--tags",
  "--long",
  "--first-parent",
]
local_scheme = "no-local-version"
root = ".."
[tool.hatch.metadata]
allow-direct-references = true
[tool.hatch.build.targets.wheel]
only-include = ["src/openllm_client"]
sources = ["src"]
[tool.hatch.build.targets.sdist]
exclude = ["/.git_archival.txt", "tests", "/.python-version-default"]
[tool.hatch.build.targets.wheel.hooks.mypyc]
dependencies = [
  "hatch-mypyc==0.16.0",
  "mypy==1.5.1",
  # avoid https://github.com/pallets/click/issues/2558
  "click==8.1.3",
  "bentoml==1.1.2",
  "transformers>=4.32.1",
  "pandas-stubs",
  "types-psutil",
  "types-tabulate",
  "types-PyYAML",
  "types-protobuf",
]
enable-by-default = false
include = ["src/openllm_client/__init__.py", "src/openllm_client/client.py"]
# NOTE: This is consistent with pyproject.toml
mypy-args = [
  "--strict",
  # this is because all transient library doesn't have types
  "--allow-subclassing-any",
  "--check-untyped-defs",
  "--ignore-missing-imports",
  "--no-warn-return-any",
  "--warn-unreachable",
  "--no-warn-no-return",
  "--no-warn-unused-ignores",
]
options = { verbose = true, strip_asserts = true, debug_level = "2", opt_level = "3", include_runtime_files = true }
require-runtime-dependencies = true

[tool.hatch.metadata.hooks.fancy-pypi-readme]
content-type = "text/markdown"
# PyPI doesn't support the <picture> tag.
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
text = """
<p align="center">
  <a href="https://github.com/bentoml/openllm">
    <img src="https://raw.githubusercontent.com/bentoml/openllm/main/.github/assets/main-banner.png" alt="Banner for OpenLLM" />
  </a>
</p>

"""
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
end-before = "\n<!-- hatch-fancy-pypi-readme intro stop -->"
path = "README.md"
start-after = "<!-- hatch-fancy-pypi-readme intro start -->\n"
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
text = """

<p align="center">
  <img src="https://raw.githubusercontent.com/bentoml/openllm/main/.github/assets/output.gif" alt="Gif showing OpenLLM Intro" />
</p>
"""
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
text = """

<p align="center">
  <img src="https://raw.githubusercontent.com/bentoml/openllm/main/.github/assets/agent.gif" alt="Gif showing Agent integration" />
</p>
"""
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
end-before = "\n<!-- hatch-fancy-pypi-readme interim stop -->"
path = "README.md"
start-after = "<!-- hatch-fancy-pypi-readme interim start -->\n"
[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]
text = """

---

[Click me for full changelog](https://github.com/bentoml/openllm/blob/main/CHANGELOG.md)
"""
