#### Fine-tuning API with `LLM.tuning`

Users should be able to easily fine tune any supported model architecture with
`LLM.tuning`

```python
llm = openllm.AutoLLM.for_model("opt", model_id='facebook/opt-6.7b')

# prepare your training dataset

llm.prepare_for_training(adapter_type='lora', r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"], lora_dropout=0.05, bias="none")

llm.tuning(train_dataset=dataset_dict["train"],
        args=dict(
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            max_steps=50,
            learning_rate=3e-4,
            fp16=True,
            logging_steps=1,
        ),
)
```

The output of the LoRA weights can be saved into
`$XDG_CACHE_HOME/openllm/fine_tune/opt--facebook-opt-6.7b/<version>/`

If users want to specify their own output path, one can do so with passing
`output_dir` to `llm.tuning(args)`:

```python
llm.tuning(train_dataset=dataset_dict["train"],
        args=dict(
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            max_steps=50,
            learning_rate=3e-4,
            fp16=True,
            logging_steps=1,
            output_dir="/path/to/output",
        ),
)
```

Then the given fine tuning weights can be served with the model via
`openllm start`:

```bash
export OPENLLM_OPT_FINE_TUNE_PATH=$(openllm fine-tune get opt)
openllm start opt --model-id facebook/opt-6.7b
```

If you just wish to try some pretrained adapter checkpoint, you can use
`--adapter-id` and optional `--adapter-name`:

```bash
openllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6.7b-lora
```

It also supports multiple LoRA layers with multiple adapter name

```bash
openllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6.7b-lora --adapter-name eng_lora --adapter-id aarnphm/opt-5.6b-vn-lora --adapter-name vn_lora
```

By default, the first `adapter-id` will be the default lora layer, but
optionally users can change what lora layer to use for inference via
`/v1/adapters`:

```bash
curl -X POST http://localhost:3000/v1/adapters --json '{"adapter_name": "vn_lora"}'
```

> Note that for multiple `adapter-name` and `adapter-id`, it is recomended to
> update to use the default adapter before sending the inference, to avoid any
> performance degradation

To include this into the Bento, one can also provide a `--adapter-id` into
`openllm build`:

```bash
openllm build opt --model-id facebook/opt-6.7b --adapter-id $(openllm fine-tune get opt)
```
