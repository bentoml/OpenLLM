Serving LLM with fine-tuned LoRA, QLoRA adapters layers

Then the given fine tuning weights can be served with the model via
`openllm start`:

```bash
openllm start opt --model-id facebook/opt-6.7b --adapter-id /path/to/adapters
```

If you just wish to try some pretrained adapter checkpoint, you can use
`--adapter-id`:

```bash
openllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6.7b-lora
```

To use multiple adapters, use the following format:

```bash
openllm start opt --model-id facebook/opt-6.7b --adapter-id aarnphm/opt-6.7b-lora --adapter-id aarnphm/opt-6.7b-lora:french_lora
```

By default, the first `adapter-id` will be the default lora layer, but
optionally users can change what lora layer to use for inference via
`/v1/adapters`:

```bash
curl -X POST http://localhost:3000/v1/adapters --json '{"adapter_name": "vn_lora"}'
```

> Note that for multiple `adapter-name` and `adapter-id`, it is recomended to
> update to use the default adapter before sending the inference, to avoid any
> performance degradation

To include this into the Bento, one can also provide a `--adapter-id` into
`openllm build`:

```bash
openllm build opt --model-id facebook/opt-6.7b --adapter-id ...
```

Separate out configuration builder, to make it more flexible for future
configuration generation.
