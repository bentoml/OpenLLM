APIs for LLMService are now provisional based on the capabilities of the LLM.

The following APIs are considered provisional:

- `/v1/embeddings`: This will be available if the LLM supports embeddings (i.e: ``LLM.embeddings`` is implemented. Example model are ``llama``)
- `/hf/agent`: This will be available if LLM supports running HF agents (i.e: ``LLM.generate_one`` is implemented. Example model are ``starcoder``, ``falcon``.)
- `POST /v1/adapters` and `GET /v1/adapters`: This will be available if the server is running with LoRA weights

``openllm.LLMRunner`` now include three additional boolean:
- `runner.supports_embeddings`: Whether this runner supports embeddings
- `runner.supports_hf_agent`: Whether this runner support HF agents
- `runner.has_adapters`: Whether this runner is loaded with LoRA adapters.

Optimized ``openllm.models``'s bytecode performance
