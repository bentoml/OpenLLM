Now, OpenLLM is compatible with BentoML 1.2 and above architecture.

Additionally, `openllm` CLI will only offer `start` and `build` to simplify the workflow.

OpenLLM will also now require vllm by default, and CPU support is currently turning off. We will look into supporting CPU in later version as our main focus is on accelerator.

Python API is also considered deprecated and internal only. If you are using this in your old service, make sure to set `IMPLEMENTATION=deprecated` as environment variable to avoid breaking changes. We recommend users to upgrade to BentoML 1.2.
