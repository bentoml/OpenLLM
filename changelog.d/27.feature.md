Added support for quantization during serving time.
`openllm start` now support `--quantize 8bit` and `--quantize 4bit`
`GPTQ` quantization support is on the roadmap and currently
being worked on.
`openllm start` now also support `--bettertransformer` to use
`BetterTransformer` for serving
Refactored `openllm.LLMConfig` to be able to use with `__getitem__`
to acecss the config value: `openllm.DollyV2Config()['requirements']`
the order being: `__openllm_*__ > self.<key> > __openllm_generation_class__ > __openllm_extras__`
Added `towncrier` workflow to easily generate changelog entries
Added `use_pipeline`, `bettertransformer` flag into ModelSettings
`LLMConfig` now supported `__dataclass_transform__` protocol to help
with type-checking
Changed `openllm download-models` to `openllm download`
