Added support for base container with OpenLLM. The base container will contains all necessary requirements
to run OpenLLM. Currently it does included compiled version of FlashAttention v2, vLLM, AutoGPTQ and triton.

This will now be the base image for all future BentoLLM. The image will also be published to public GHCR.

To extend and use this image into your bento, simply specify ``base_image`` under ``bentofile.yaml``:

```yaml
docker:
  base_image: ghcr.io/bentoml/openllm:<hash>
```

The release strategy would include:
- versioning of ``ghcr.io/bentoml/openllm:<sha1>`` for every commit to main, ``ghcr.io/bentoml/openllm:v0.2.11`` for specific release version
- alias ``ghcr.io/bentoml/openllm:latest`` will point to the latest stable release, whereas ``ghcr.io/bentoml/openllm:nightly`` will point to main.

Note that all these images include compiled kernels that has been tested on Ampere GPUs with CUDA 11.8.

To quickly run the image, do the following:

```bash
docker run --rm --gpus all -it -v /home/ubuntu/.local/share/bentoml:/tmp/bentoml -e BENTOML_HOME=/tmp/bentoml \
            -e OPENLLM_USE_LOCAL_LATEST=True -e OPENLLM_LLAMA_FRAMEWORK=vllm ghcr.io/bentoml/openllm:2b5e96f90ad314f54e07b5b31e386e7d688d9bb2 start llama --model-id meta-llama/Llama-2-7b-chat-hf --workers-per-resource conserved --debug`
```

In conjunction with this, OpenLLM now also have a set of small CLI utilities via ``openllm ext`` for ease-of-use

``openllm build`` will now include a new option, ``--base-image-strategies``, which sets to choose either ``edge`` (always points to ``main``)
or ``stable`` (always point to latest release, starting 0.2.11)

General fixes around codebase bytecode optimization

Fixes logs output to filter correct level based on ``--debug`` and ``--quiet``
