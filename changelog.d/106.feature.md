Added support for GPTNeoX models. All variants of GPTNeoX, including Dolly-V2
and StableLM can now also use `openllm start gpt-neox`

`openllm models -o json` nows return CPU and GPU field. `openllm models` now
show table that mimics the one from README.md

Added scripts to automatically add models import to `__init__.py`

`--workers-per-resource` now accepts the following strategies:

- `round_robin`: Similar behaviour when setting `--workers-per-resource 1`. This
  is useful for smaller models.
- `conserved`: This will determine the number of available GPU resources, and
  only assign one worker for the LLMRunner with all available GPU resources. For
  example, if ther are 4 GPUs available, then `conserved` is equivalent to
  `--workers-per-resource 0.25`.
