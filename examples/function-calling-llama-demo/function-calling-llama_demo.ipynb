{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1HOaFA1ogMDPalGw_e55VcfDhiiblzrFO?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz05mlk7-JXh"
      },
      "source": [
        "## Setup and Install\n",
        "- It's best to run fLlama on a GPU, which you can do using a free Colab notebook.\n",
        "- Check the Google Colab runtime to the top right corner.\n",
        "- Or, go to the menu -> Runtime -> Change Runtime Type.\n",
        "- Select GPU (T4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIZef-cjRkgR",
        "outputId": "a9cec78e-6c5b-4282-bd00-06abc1f5dd01"
      },
      "outputs": [],
      "source": [
        "##@title Clone the project and install dependencies\n",
        "UPDATE_OPENLLM = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'OpenLLM'\n",
        "PROJECT_NAME = 'examples/fLlama-demo'\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup OpenLLM =- && git clone https://github.com/bentoml/OpenLLM.git\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if UPDATE_OPENLLM:\n",
        "  !echo -= Updating openllm =-\n",
        "  !git fetch origin feat/llama-colab && git checkout feat/llama-colab && git pull\n",
        "\n",
        "%cd {PROJECT_NAME}\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "![ -f requirements.txt ] && pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3AHWrxqA0mP",
        "outputId": "bacb2478-f350-4ecd-8c20-da184093358a"
      },
      "outputs": [],
      "source": [
        "#@title [optional] Check the memory, gpu\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || ture\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    #print(\"RUN `openllm models` to find modles which can runable on CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5dpzGppM775"
      },
      "source": [
        "## download model from huggingface\n",
        "https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTIISRK_Rx9X",
        "outputId": "e4580fdb-5b2e-4606-d5ba-f7d5600ae1ef"
      },
      "outputs": [],
      "source": [
        "#@title bentoml runner defined in runner.py, execute download_model to download the model\n",
        "!python runner.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8DKXm7Ds9au",
        "outputId": "5958bc94-8cf1-491c-c57d-08c1644d4db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m[\u001b[0m\n",
            "  \u001b[1m{\u001b[0m\n",
            "    \u001b[1;34m\"tag\"\u001b[0m: \u001b[32m\"vllm-trelis--llama-2-7b-chat-hf-function-calling-v2:7013579ab9c47d409e0c9c02a03de4027a65e566\"\u001b[0m,\n",
            "    \u001b[1;34m\"module\"\u001b[0m: \u001b[32m\"openllm.serialisation.transformers\"\u001b[0m,\n",
            "    \u001b[1;34m\"size\"\u001b[0m: \u001b[32m\"12.55 GiB\"\u001b[0m,\n",
            "    \u001b[1;34m\"creation_time\"\u001b[0m: \u001b[32m\"2023-09-26 11:11:16\"\u001b[0m\n",
            "  \u001b[1m}\u001b[0m\n",
            "\u001b[1m]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title check the model we just download\n",
        "!bentoml models list -o json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8MBtsGCOzt"
      },
      "source": [
        "## Prepare the prompt for the fLlama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "hhTgrvTzyZq9",
        "outputId": "ca8f999d-bb3e-4d0e-f56b-7972e30c98aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<FUNCTIONS>{\\n    \"function\": \"HVAC_CONTROL\",\\n    \"description\": \"Call an API to adjust the AC setting in the car.\",\\n    \"arguments\": [\\n        {\\n            \"name\": \"action\",\\n            \"description\": \"The type of action requested, must be one of the following:\\\\n\\'SET_TEMPERATURE\\': set, increase, decrease or turn on AC to a desired temperature. Must be used with the temperature argument;\\\\n\\'UP\\': increase the temperature from current setting. If a specific temperature is given, use SET_TEMPERATURE instead;\\\\n\\'DOWN\\': decrease the temperature from current setting. If a specific temperature is given, use SET_TEMPERATURE instead;\\\\n\\'ON\\': turn on the AC;\\\\n\\'OFF\\': turn off the AC;\\\\n            \",\\n            \"enum\": [\\n                \"ON\",\\n                \"OFF\",\\n                \"UP\",\\n                \"DOWN\",\\n                \"SET_TEMPERATURE\"\\n            ],\\n            \"type\": \"string\"\\n        },\\n        {\\n            \"name\": \"temperature\",\\n            \"type\": \"number\",\\n            \"description\": \"Only used together with the type argument is SET_TEMPERATURE\"\\n        }\\n    ]\\n}</FUNCTIONS>\\n\\n[INST] <<SYS>>\\nyou are a helpful AI assitent in a car, that respond to the driver\\'s instructions and helps setting the temperature in the vehicle. You can only communicate using JSON.\\n<</SYS>>\\n\\nplease shut down the AC [/INST]\\n\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Build a sample prompt query\n",
        "from prompt import *\n",
        "prompt_template(HVAC_FUNCS)(\"please shut down the AC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iirE4iCXCohO"
      },
      "source": [
        "## [Optional] Start the server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TkIjPLCcugni",
        "outputId": "577abd8a-5dcf-4b84-8023-9ece9a2e25df"
      },
      "outputs": [],
      "source": [
        "#@title Start the llama server locally using `bentoml` command.\n",
        "#bentoml serve will use the service.py under this folder to start a llama2 server\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  #using colab proxy URL\n",
        "  from google.colab.output import eval_js\n",
        "  print(\"you are in colab runtime. please try it out in %s\" % eval_js(\"google.colab.kernel.proxyPort(8001)\"))\n",
        "\n",
        "RUN_IN_BACKGROUND = False #@param {type:\"boolean\"}\n",
        "if RUN_IN_BACKGROUND:\n",
        "  !nohup bentoml serve service:svc -p 8001 &\n",
        "else:\n",
        "  !bentoml serve service:svc -p 8001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSLpf-zWIKal",
        "outputId": "a55a7738-327b-42dc-ed09-a5324fc61766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not able to process the request in 60 seconds"
          ]
        }
      ],
      "source": [
        "#@title [Optional] if you run server in background, you can test it in colab env\n",
        "!curl -X 'POST' \\\n",
        "  'http://127.0.0.1:8001/query' \\\n",
        "  -H 'accept: application/json' \\\n",
        "  -H 'Content-Type: text/plain' \\\n",
        "  -d 'i feel a little bit cold'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGzNsnklOoH2"
      },
      "source": [
        "## build bento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2q6lmJFOzSz",
        "outputId": "8cf4a810-7ce7-4312-ee8e-b69fedd1f69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing bentofile.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file bentofile.yaml\n",
        "service: 'service:svc'\n",
        "include:\n",
        "  - '*.py'\n",
        "python:\n",
        "  packages:\n",
        "    - openllm[llama,vllm]\n",
        "    - git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cma6KJ-fOkUx",
        "outputId": "9a65802b-9b37-49d3-fa0c-75fb353f16a3"
      },
      "outputs": [],
      "source": [
        "#@title Build the llama bento using bentoml on entofile.yaml\n",
        "!bentoml build -f bentofile.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omQuXr5zPmrz",
        "outputId": "8ef63340-c27d-4f6c-90c7-e41fa9dd2b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m[\u001b[0m\n",
            "  \u001b[1m{\u001b[0m\n",
            "    \u001b[32m\"tag\"\u001b[0m: \u001b[32m\"assistant:nudslos4l632easc\"\u001b[0m,\n",
            "    \u001b[32m\"size\"\u001b[0m: \u001b[32m\"19.07 KiB\"\u001b[0m,\n",
            "    \u001b[32m\"creation_time\"\u001b[0m: \u001b[32m\"2023-09-26 11:28:36\"\u001b[0m\n",
            "  \u001b[1m}\u001b[0m\n",
            "\u001b[1m]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Check the bentos you just build\n",
        "! bentoml list -o json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnrL-Yk2PzjU",
        "outputId": "c8c867d6-5d0c-4dfb-bbad-6d9c7051f078"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/bentoml/BentoML.git@fix/push_oom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JfshuvZPbhM",
        "outputId": "635a351a-bcdb-493a-fb07-a34c4fb67004"
      },
      "outputs": [],
      "source": [
        "#@title Push them to bentocloud if you want\n",
        "! bentoml list -o json\n",
        "return_code = !bentoml cloud list-context\n",
        "print(return_code)\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  #login bentocloud\n",
        "  endpoint = input(\"input endpoint (like https://xxx.cloud.bentoml.com):\")\n",
        "  token = input(\"input token (please follow https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html#creating-an-api-token):\")\n",
        "  !bentoml cloud login --api-token {token} --endpoint {endpoint} --context colab-user\n",
        "\n",
        "#change to your own bentos tag\n",
        "!bentoml push assistant:nudslos4l632easc  --context colab-user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BK0GVY2Syqz"
      },
      "outputs": [],
      "source": [
        "#@title Follow the [guide](https://www.bentoml.com/blog/deploying-llama-2-7b-on-bentocloud) to deploy this llama model on bentocloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1204WYoS6uh"
      },
      "outputs": [],
      "source": [
        "#@title Or use bentoml client sdk to start a deployment\n",
        "import bentoml\n",
        "import json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  print(\"please login first!\")\n",
        "else:\n",
        "  client = bentoml.cloud.BentoCloudClient()\n",
        "  #detailed configuration in https://docs.bentoml.com/en/latest/bentocloud/reference/deployment-creation-and-update-info.html\n",
        "  #runner config\n",
        "  runner = bentoml.cloud.Resource.for_runner(\n",
        "      resource_instance=\"starter-aws-g4dn-xlarge-gpu-t4-xlarge\",\n",
        "      hpa_conf={\"min_replicas\": 1, \"max_replicas\": 1},\n",
        "  )\n",
        "  #api-server hpa config\n",
        "  api_server = bentoml.cloud.Resource.for_api_server(\n",
        "      resource_instance=\"starter-aws-t3-2xlarge-cpu-small\",\n",
        "  )\n",
        "  hpa_conf = bentoml.cloud.Resource.for_hpa_conf(min_replicas=1, max_replicas=1)\n",
        "\n",
        "  res = client.deployment.create(\n",
        "      deployment_name=\"test-codellama\",\n",
        "      bento=\"assistant:nudslos4l632easc\",\n",
        "      context = \"colab-user\",\n",
        "      cluster_name = \"default\",\n",
        "      mode=\"deployment\",\n",
        "      kube_namespace='yatai',\n",
        "      runners_config={\"llm-llama-runner\": runner},\n",
        "      api_server_config=api_server,\n",
        "      hpa_conf=hpa_conf,\n",
        "  )\n",
        "  print(json.dumps(res, indent=4))\n",
        "  #!bentoml deployment create -f deployment.json --context colab-user"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
