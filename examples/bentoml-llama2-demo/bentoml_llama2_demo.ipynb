{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza2KMZA805r"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/158zwSM__zs0caehysLinxLkjY7_naqcK?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz2x8naaLWzN"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "UPDATE_OPENLLM = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'OpenLLM'\n",
        "PROJECT_NAME = 'examples/bentoml-llama2-demo'\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup OpenLLM =- && git clone https://github.com/bentoml/OpenLLM.git\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if UPDATE_OPENLLM:\n",
        "  !echo -= Updating openllm =-\n",
        "  !git fetch origin feat/llama-colab && git checkout feat/llama-colab && git pull\n",
        "\n",
        "%cd {PROJECT_NAME}\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "![ -f requirements.txt ] && pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei23zHY-NHhh"
      },
      "outputs": [],
      "source": [
        "#@title [optional] Check the memory, and gpu info you have\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || ture\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    #print(\"RUN `openllm models` to find modles which can runable on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-6Af0GhhmNj"
      },
      "outputs": [],
      "source": [
        "#@title Download llama2 model\n",
        "!python runner.py\n",
        "#list the model you just download\n",
        "!bentoml models list -o json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpoLNTlUPHuG"
      },
      "outputs": [],
      "source": [
        "#@title [optional] Start the llama server locally using `bentoml` command.\n",
        "#bentoml serve will use the service.py under this folder to start a llama2 server\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  #using colab proxy URL\n",
        "  from google.colab.output import eval_js\n",
        "  print(\"you are in colab runtime. please try it out in %s\" % eval_js(\"google.colab.kernel.proxyPort(8001)\"))\n",
        "\n",
        "!bentoml serve service:svc -p 8001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAdxFN6wXpHI"
      },
      "outputs": [],
      "source": [
        "#@title Build the llama bento using bentoml (service.py bentofile.yaml)\n",
        "!bentoml build -f bentofile.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuWN1pksDu_U"
      },
      "outputs": [],
      "source": [
        "#@title Check the bentos you just build, and push them to bentocloud if you want\n",
        "! bentoml list -o json\n",
        "return_code = !bentoml cloud list-context\n",
        "print(return_code)\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  #login bentocloud\n",
        "  endpoint = input(\"input endpoint (like https://xxx.cloud.bentoml.com):\")\n",
        "  token = input(\"input token (please follow https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html#creating-an-api-token):\")\n",
        "  !bentoml cloud login --api-token {token} --endpoint {endpoint} --context colab-user\n",
        "\n",
        "#change to your own bentos tag\n",
        "!bentoml push llama-service:zhmroycxp6v4casc  --context colab-user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUlyjqegVfwO"
      },
      "outputs": [],
      "source": [
        "#@title Follow the [guide](https://www.bentoml.com/blog/deploying-llama-2-7b-on-bentocloud) to deploy this llama model on bentocloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkkd4pmEaVK0"
      },
      "outputs": [],
      "source": [
        "#@title Or use bentoml client to start a deployment (config specified by deployment.json)\n",
        "import bentoml\n",
        "import json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  print(\"please login first!\")\n",
        "else:\n",
        "  client = bentoml.cloud.BentoCloudClient()\n",
        "  #detailed configuration in https://docs.bentoml.com/en/latest/bentocloud/reference/deployment-creation-and-update-info.html\n",
        "  #runner config\n",
        "  runner = bentoml.cloud.Resource.for_runner(\n",
        "      resource_instance=\"starter-aws-g4dn-xlarge-gpu-t4-xlarge\",\n",
        "      #hpa_conf={\"min_replicas\": 1, \"max_replicas\": 1},\n",
        "  )\n",
        "  #api-server hpa config\n",
        "  api_server = bentoml.cloud.Resource.for_api_server(\n",
        "      resource_instance=\"starter-aws-t3-2xlarge-cpu-small\",\n",
        "  )\n",
        "  hpa_conf = bentoml.cloud.Resource.for_hpa_conf(min_replicas=1, max_replicas=1)\n",
        "\n",
        "  res = client.deployment.create(\n",
        "      deployment_name=\"test-llama2\",\n",
        "      bento=\"llama-service:zhmroycxp6v4casc\",\n",
        "      context = \"colab-user\",\n",
        "      cluster_name = \"default\",\n",
        "      mode=\"deployment\",\n",
        "      kube_namespace='yatai',\n",
        "      runners_config={\"llm-llama-runner\": runner},\n",
        "      api_server_config=api_server,\n",
        "      hpa_conf=hpa_conf,\n",
        "  )\n",
        "  print(json.dumps(res, indent=4))\n",
        "  #!bentoml deployment create -f deployment.json --context colab-user"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
