{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8rezQ6tVJI"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1NLFaFNxNxVgLPVgRevGv3NeyzJadNw96?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6PG28SsdvA",
        "outputId": "bf848a93-27d7-4089-9e02-79accc1ead20"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "!pip install openllm[llama] bentoml vllm accelerate bitsandbytes --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnDCYgl0tm0X",
        "outputId": "77574066-2bb3-49ec-b786-d10f74f3076e"
      },
      "outputs": [],
      "source": [
        "#@title [optional] Check the memory, and gpu info you have\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || ture\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    #print(\"RUN `openllm models` to find modles which can runable on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BN1C6-81trlS",
        "outputId": "422fa35f-14ab-4c99-8ac7-689f834a9a81"
      },
      "outputs": [],
      "source": [
        "#@title [Optional] start the llama2 server locally using openllm\n",
        "#RUN `openllm build -h` for help\n",
        "import sys\n",
        "\n",
        "#run `openllm models` to find more model IDs of llama2\n",
        "MODEL_ID = \"NousResearch/llama-2-7b-chat-hf\"  #@param [\"NousResearch/llama-2-7b-chat-hf\", \"NousResearch/llama-2-13b-chat-hf\",\"NousResearch/llama-2-70b-chat-hf\"]\n",
        "if 'google.colab' in sys.modules:\n",
        "  #using colab proxy URL\n",
        "  from google.colab.output import eval_js\n",
        "  print(\"you are in colab runtime. please try it out in %s\" % eval_js(\"google.colab.kernel.proxyPort(8001)\"))\n",
        "\n",
        "! openllm start llama --model-id {MODEL_ID} --backend pt --port 8001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKpvHxGNwhIK",
        "outputId": "c021ca80-8744-4c3b-92f2-001eba292fe4"
      },
      "outputs": [],
      "source": [
        "#@title Build bentos locally using Openllm\n",
        "#RUN `openllm build -h` for help\n",
        "#!pwd && du -sh *\n",
        "!openllm build llama --model-id NousResearch/llama-2-7b-chat-hf --backend pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTMSjC_71Xk6",
        "outputId": "cede885a-4f49-42f0-fb63-f00124b472be"
      },
      "outputs": [],
      "source": [
        "#@title Check the bentos you just build, and push them to bentocloud\n",
        "!bentoml list -o json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  #login bentocloud\n",
        "  endpoint = input(\"input endpoint (like https://xxx.cloud.bentoml.com): \")\n",
        "  token = input(\"input token (please follow https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html#creating-an-api-token):\")\n",
        "  !bentoml cloud login --api-token {token} --endpoint {endpoint} --context colab-user\n",
        "\n",
        "#change to your own bentos tag\n",
        "!bentoml push nousresearch--llama-2-7b-chat-hf-service:37892f30c23786c0d5367d80481fa0d9fba93cf8 --context colab-user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NN1rOoNVcma"
      },
      "outputs": [],
      "source": [
        "#@title Follow the [guide](https://www.bentoml.com/blog/deploying-llama-2-7b-on-bentocloud) to deploy this llama model on bentocloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLUH1c6rcE47"
      },
      "outputs": [],
      "source": [
        "#@title Or use bentoml client to start a deployment\n",
        "import bentoml\n",
        "import json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  print(\"please login first!\")\n",
        "else:\n",
        "  client = bentoml.cloud.BentoCloudClient()\n",
        "  #detailed configuration in https://docs.bentoml.com/en/latest/bentocloud/reference/deployment-creation-and-update-info.html\n",
        "  #runner config\n",
        "  runner = bentoml.cloud.Resource.for_runner(\n",
        "      resource_instance=\"starter-aws-g4dn-xlarge-gpu-t4-xlarge\",\n",
        "      #hpa_conf={\"min_replicas\": 1, \"max_replicas\": 1},\n",
        "  )\n",
        "  #api-server hpa config\n",
        "  api_server = bentoml.cloud.Resource.for_api_server(\n",
        "      resource_instance=\"starter-aws-t3-2xlarge-cpu-small\",\n",
        "  )\n",
        "  hpa_conf = bentoml.cloud.Resource.for_hpa_conf(min_replicas=1, max_replicas=1)\n",
        "\n",
        "  res = client.deployment.create(\n",
        "      deployment_name=\"test-llama2\",\n",
        "      bento=\"nousresearch--llama-2-7b-chat-hf-service:37892f30c23786c0d5367d80481fa0d9fba93cf8\",\n",
        "      context = \"colab-user\",\n",
        "      cluster_name = \"default\",\n",
        "      #mode=\"deployment\",\n",
        "      kube_namespace='yatai',\n",
        "      runners_config={\"llm-llama-runner\": runner},\n",
        "      api_server_config=api_server,\n",
        "      hpa_conf=hpa_conf,\n",
        "  )\n",
        "  print(json.dump(res, indent=4))\n",
        "  #!bentoml deployment create -f deployment.json --context colab-user"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
