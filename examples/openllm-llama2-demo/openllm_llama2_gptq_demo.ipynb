{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8rezQ6tVJI"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1_8CWyOEpMH0eQzY9l1wc1OevTJf0VVXb?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6PG28SsdvA",
        "outputId": "84f6c80e-ee53-45bd-d0ed-dc4a6eb09f35"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "!pip install openllm[llama] bentoml vllm accelerate bitsandbytes --upgrade -q\n",
        "!pip install \"openllm[gptq]\" --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnDCYgl0tm0X",
        "outputId": "ffd4111e-ebaf-45fc-dd5b-4d384eb6e180"
      },
      "outputs": [],
      "source": [
        "#@title [optional] Check the memory, and gpu info you have\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || ture\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    #print(\"RUN `openllm models` to find modles which can runable on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BN1C6-81trlS",
        "outputId": "211495cb-3a3f-4318-9f99-85a875deaffd"
      },
      "outputs": [],
      "source": [
        "#@title [Optional] start the llama2 server locally using openllm\n",
        "#RUN `openllm build -h` for help\n",
        "import sys\n",
        "\n",
        "#run `openllm models` to find more model IDs of llama2\n",
        "MODEL_ID = \"TheBloke/Llama-2-13B-chat-GPTQ\" #@param [\"TheBloke/Llama-2-7b-Chat-GPTQ\", \"TheBloke/Llama-2-13B-chat-GPTQ\", \"TheBloke/Llama-2-70B-chat-GPTQ\"]\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  #using colab proxy URL\n",
        "  from google.colab.output import eval_js\n",
        "  print(\"you are in colab runtime. please try it out in %s\" % eval_js(\"google.colab.kernel.proxyPort(8001)\"))\n",
        "\n",
        "! openllm start llama --model-id {MODEL_ID} --backend pt  --quantize gptq --port 8001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKpvHxGNwhIK",
        "outputId": "135e600f-45b1-4bae-caa3-a5a10455d6ae"
      },
      "outputs": [],
      "source": [
        "#@title Build bentos locally using Openllm\n",
        "#RUN `openllm build -h` for help\n",
        "!openllm build llama --model-id TheBloke/Llama-2-13B-chat-GPTQ --backend pt --quantize gptq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTMSjC_71Xk6",
        "outputId": "eda0a9ac-1bf7-4cab-b35e-46245a6d6cca"
      },
      "outputs": [],
      "source": [
        "#@title Check the bentos you just build, and push them to bentocloud\n",
        "!bentoml list -o json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  #login bentocloud\n",
        "  endpoint = input(\"input endpoint (like https://xxx.cloud.bentoml.com): \")\n",
        "  token = input(\"input token (please follow https://docs.bentoml.com/en/latest/bentocloud/how-tos/manage-access-token.html#creating-an-api-token):\")\n",
        "  !bentoml cloud login --api-token {token} --endpoint {endpoint} --context colab-user\n",
        "\n",
        "#change to your own bentos tag\n",
        "!bentoml push thebloke--llama-2-13b-chat-gptq-service:ec124ec7c8f14b67b0808b870b08497ce27634fa --context colab-user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NN1rOoNVcma"
      },
      "outputs": [],
      "source": [
        "#@title Follow the [guide](https://www.bentoml.com/blog/deploying-llama-2-7b-on-bentocloud) to deploy this llama model on bentocloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDTkCCxMDVk-",
        "outputId": "207d9b19-b582-44d8-f9a5-fd43441678d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m[\u001b[0m\n",
            "  \u001b[1m{\u001b[0m\n",
            "    \u001b[32m\"tag\"\u001b[0m: \n",
            "\u001b[32m\"thebloke--llama-2-13b-chat-gptq-service:ec124ec7c8f14b67b0808b870b08497ce27634f\u001b[0m\n",
            "\u001b[32ma\"\u001b[0m,\n",
            "    \u001b[32m\"size\"\u001b[0m: \u001b[32m\"34.48 KiB\"\u001b[0m,\n",
            "    \u001b[32m\"creation_time\"\u001b[0m: \u001b[32m\"2023-09-20 09:58:35\"\u001b[0m\n",
            "  \u001b[1m}\u001b[0m\n",
            "\u001b[1m]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!bentoml list -o json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWrrwsw1CGXF",
        "outputId": "1c4f9b4a-c616-488a-9908-cde46d21972a"
      },
      "outputs": [],
      "source": [
        "#@title Or use bentoml client to start a deployment\n",
        "import bentoml\n",
        "import json\n",
        "\n",
        "return_code = !bentoml cloud list-context\n",
        "if \"colab-user\" not in ''.join(return_code):\n",
        "  print(\"please login first!\")\n",
        "else:\n",
        "  client = bentoml.cloud.BentoCloudClient()\n",
        "  #detailed configuration in https://docs.bentoml.com/en/latest/bentocloud/reference/deployment-creation-and-update-info.html\n",
        "  #runner config\n",
        "  runner = bentoml.cloud.Resource.for_runner(\n",
        "      resource_instance=\"starter-aws-g4dn-xlarge-gpu-t4-xlarge\",\n",
        "      hpa_conf={\"min_replicas\": 1, \"max_replicas\": 1},\n",
        "  )\n",
        "  #api-server hpa config\n",
        "  api_server = bentoml.cloud.Resource.for_api_server(\n",
        "      resource_instance=\"starter-aws-t3-2xlarge-cpu-small\",\n",
        "  )\n",
        "  hpa_conf = bentoml.cloud.Resource.for_hpa_conf(min_replicas=1, max_replicas=1)\n",
        "\n",
        "  res = client.deployment.create(\n",
        "      deployment_name=\"test-thebloke\",\n",
        "      bento=\"thebloke--llama-2-13b-chat-gptq-service:ec124ec7c8f14b67b0808b870b08497ce27634fa\",\n",
        "      context = \"colab-user\",\n",
        "      cluster_name = \"default\",\n",
        "      #mode=\"function\",\n",
        "      kube_namespace='yatai',\n",
        "      runners_config={\"llm-llama-runner\": runner},\n",
        "      api_server_config=api_server,\n",
        "      hpa_conf=hpa_conf,\n",
        "  )\n",
        "  print(json.dump(res, indent=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
