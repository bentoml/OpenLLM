# NOTE: The following are managed by ./tools/dependencies.py
# project.classifiers, project.dependencies, project.optional-dependencies, project.urls
[build-system]
build-backend = "hatchling.build"
requires = ["hatchling", "hatch-vcs", "hatch-fancy-pypi-readme", "hatch-mypyc==0.16.0"]

[project]
authors = [{ name = "Aaron Pham", email = "aarnphm@bentoml.com" }]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Environment :: GPU :: NVIDIA CUDA",
    "Environment :: GPU :: NVIDIA CUDA :: 12",
    "Environment :: GPU :: NVIDIA CUDA :: 11.8",
    "Environment :: GPU :: NVIDIA CUDA :: 11.7",
    "License :: OSI Approved :: Apache Software License",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries",
    "Operating System :: OS Independent",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Intended Audience :: System Administrators",
    "Typing :: Typed",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3 :: Only",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]
dependencies = [
    "bentoml[grpc,io]>=1.0.25",
    "transformers[torch,tokenizers,accelerate]>=4.29.0",
    "safetensors",
    "optimum",
    "attrs>=23.1.0",
    "cattrs>=23.1.0",
    "orjson",
    "inflection",
    "tabulate[widechars]>=0.9.0",
    "httpx",
    "click>=8.1.3",
    "typing_extensions",
    "mypy_extensions",
    "ghapi",
    "cuda-python;platform_system!=\"Darwin\"",
    "bitsandbytes<0.42",
]
description = 'OpenLLM: Operating LLMs in production'
dynamic = ["version", "readme"]
keywords = [
    "MLOps",
    "AI",
    "BentoML",
    "Model Serving",
    "Model Deployment",
    "LLMOps",
    "Large Language Model",
    "Generative AI",
    "StableLM",
    "Alpaca",
    "PyTorch",
    "Transformers",
]
license = "Apache-2.0"
name = "openllm"
requires-python = ">=3.8"

[project.scripts]
openllm = "openllm.cli.entrypoint:cli"
openllm-build-base-container = "openllm.cli.extension.build_base_container:cli"
openllm-dive-bentos = "openllm.cli.extension.dive_bentos:cli"
openllm-get-containerfile = "openllm.cli.extension.get_containerfile:cli"
openllm-get-prompt = "openllm.cli.extension.get_prompt:cli"
openllm-list-bentos = "openllm.cli.extension.list_bentos:cli"
openllm-list-models = "openllm.cli.extension.list_models:cli"
openllm-playground = "openllm.cli.extension.playground:cli"

[project.urls]
Blog = "https://modelserving.com"
Chat = "https://discord.gg/openllm"
Documentation = "https://github.com/bentoml/openllm#readme"
GitHub = "https://github.com/bentoml/openllm"
History = "https://github.com/bentoml/openllm/blob/main/CHANGELOG.md"
Homepage = "https://bentoml.com"
Tracker = "https://github.com/bentoml/openllm/issues"
Twitter = "https://twitter.com/bentomlai"

[project.optional-dependencies]
agents = ["transformers[agents]>=4.30", "diffusers", "soundfile"]
all = [
    "openllm[agents]",
    "openllm[baichuan]",
    "openllm[chatglm]",
    "openllm[falcon]",
    "openllm[fine-tune]",
    "openllm[flan-t5]",
    "openllm[ggml]",
    "openllm[gptq]",
    "openllm[llama]",
    "openllm[mpt]",
    "openllm[openai]",
    "openllm[opt]",
    "openllm[playground]",
    "openllm[starcoder]",
    "openllm[vllm]",
]
baichuan = ["cpm-kernels", "sentencepiece"]
chatglm = ["cpm-kernels", "sentencepiece"]
falcon = ["einops", "xformers"]
fine-tune = ["peft>=0.4.0", "bitsandbytes", "datasets", "accelerate", "trl"]
flan-t5 = ["flax", "jax", "jaxlib", "tensorflow", "keras"]
ggml = ["ctransformers"]
gptq = ["auto-gptq[triton]"]
llama = ["fairscale", "sentencepiece"]
mpt = ["triton", "einops"]
openai = ["openai", "tiktoken"]
opt = ["flax", "jax", "jaxlib", "tensorflow", "keras"]
playground = ["jupyter", "notebook", "ipython", "jupytext", "nbformat"]
starcoder = ["bitsandbytes"]
vllm = ["vllm", "ray"]

[tool.pytest.ini_options]
addopts = ["-rfEX", "-pno:warnings", "--snapshot-warn-unused"]
python_files = ["test_*.py", "*_test.py"]
testpaths = ["tests"]

[tool.coverage.paths]
openllm = ["src/openllm", "*/openllm/src/openllm"]
[tool.coverage.run]
branch = true
omit = [
    "__pypackages__/*",
    "src/openllm/_version.py",
    "src/openllm/playground/",
    "src/openllm/__init__.py",
    "src/openllm/__main__.py",
    "src/openllm/utils/dummy_*.py",
    "src/openllm/_typing_compat.py",
]
source_pkgs = ["openllm"]
[tool.coverage.report]
exclude_lines = [
    "no cov",
    "pragma: no cover",
    "if __name__ == .__main__.:",
    "if t.TYPE_CHECKING:",
    "if _t.TYPE_CHECKING:",
    'if TYPE_CHECKING:',
    'if typing.TYPE_CHECKING:',
    'if t.TYPE_CHECKING and not MYPY:',
    'if DEBUG:',
    'if utils.DEBUG',
    'if openllm.utils.DEBUG',
    '@_overload',
    '@overload',
    '@t.overload',
    '@typing.overload',
    'raise NotImplementedError',
    'raise NotImplemented',
    'except MissingDependencyError:',
]
omit = [
    "__pypackages__/*",
    "src/openllm/_version.py",
    "src/openllm/playground/",
    "src/openllm/__init__.py",
    "src/openllm/__main__.py",
    "src/openllm/utils/dummy_*.py",
    "src/openllm/_typing_compat.py",
]
precision = 2
show_missing = true

[tool.hatch.version]
fallback-version = "0.0.0"
source = "vcs"
[tool.hatch.build.hooks.vcs]
version-file = "src/openllm/_version.py"
[tool.hatch.version.raw-options]
git_describe_command = ["git", "describe", "--dirty", "--tags", "--long", "--first-parent"]
local_scheme = "no-local-version"
root = ".."
[tool.hatch.metadata]
allow-direct-references = true
[tool.hatch.build.targets.wheel]
only-include = ["src/openllm"]
sources = ["src"]
[tool.hatch.build.targets.sdist]
exclude = ["/tests", "/typings"]
[tool.hatch.build.targets.wheel.hooks.mypyc]
dependencies = [
    "hatch-mypyc==0.16.0",
    "mypy==1.4.1",
    # avoid https://github.com/pallets/click/issues/2558
    "click==8.1.3",
    "bentoml==1.1.1",
    "transformers>=4.31.0",
    "pandas-stubs",
    "types-psutil",
    "types-tabulate",
    "types-PyYAML",
    "types-protobuf",
]
enable-by-default = false
include = [
    "src/openllm/bundle",
    "src/openllm/models/__init__.py",
    "src/openllm/models/auto/__init__.py",
    "src/openllm/utils/__init__.py",
    "src/openllm/utils/codegen.py",
    "src/openllm/__init__.py",
    "src/openllm/_prompt.py",
    "src/openllm/_schema.py",
    "src/openllm/_quantisation.py",
    "src/openllm/_generation.py",
    "src/openllm/_strategies.py",
    "src/openllm/exceptions.py",
    "src/openllm/testing.py",
]
# NOTE: This is consistent with pyproject.toml
mypy-args = [
    "--strict",
    # this is because all transient library doesn't have types
    "--allow-subclassing-any",
    "--follow-imports=skip",
    "--check-untyped-defs",
    "--ignore-missing-imports",
    "--no-warn-return-any",
    "--warn-unreachable",
    "--no-warn-no-return",
    "--no-warn-unused-ignores",
    "--exclude='/src\\/openllm\\/playground\\/**'",
    "--exclude='/src\\/openllm\\/_typing_compat\\.py$'",
]
options = { verbose = true, strip_asserts = true, debug_level = "2", opt_level = "3", include_runtime_files = true }
require-runtime-dependencies = true
