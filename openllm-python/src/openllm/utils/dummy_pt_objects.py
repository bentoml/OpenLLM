# This file is generated by tools/update-dummy.py. DO NOT EDIT MANUALLY!
# To update this, run ./tools/update-dummy.py
from __future__ import annotations
import typing as _t
from openllm_core.utils import DummyMetaclass as _DummyMetaclass, require_backends as _require_backends
class ChatGLM(metaclass=_DummyMetaclass):
  _backends=["torch","cpm_kernels","sentencepiece"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","cpm_kernels","sentencepiece"])
class DollyV2(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
class Falcon(metaclass=_DummyMetaclass):
  _backends=["torch","einops","xformers"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","einops","xformers"])
class FlanT5(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
class GPTNeoX(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
class Llama(metaclass=_DummyMetaclass):
  _backends=["torch","fairscale","sentencepiece","scipy"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","fairscale","sentencepiece","scipy"])
class MPT(metaclass=_DummyMetaclass):
  _backends=["torch","triton","einops"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","triton","einops"])
class OPT(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
class StableLM(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
class StarCoder(metaclass=_DummyMetaclass):
  _backends=["torch","bitsandbytes"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","bitsandbytes"])
class Baichuan(metaclass=_DummyMetaclass):
  _backends=["torch","cpm_kernels","sentencepiece"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch","cpm_kernels","sentencepiece"])
class AutoLLM(metaclass=_DummyMetaclass):
  _backends=["torch"]
  def __init__(self,*param_decls:_t.Any,**attrs: _t.Any):_require_backends(self,["torch"])
MODEL_MAPPING_NAMES:_t.Any=None
__all__:list[str]=["MODEL_MAPPING_NAMES","AutoLLM","ChatGLM","DollyV2","Falcon","FlanT5","GPTNeoX","Llama","MPT","OPT","StableLM","StarCoder","Baichuan"]
