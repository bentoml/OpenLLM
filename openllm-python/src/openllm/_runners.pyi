from typing import Any, TypeVar, Protocol, AsyncGenerator, List, Optional, Iterable, Dict
import bentoml
from _bentoml_sdk import Service
from _bentoml_sdk.service.config import ServiceConfig

from openllm_core import GenerationOutput, LLMConfig
from openllm_core._typing_compat import M, T, LiteralBackend, Unpack

from ._llm import LLM

try:
  from vllm import AsyncLLMEngine
except ImportError:
  AsyncLLMEngine = Any

try:
  from ctranslate2 import Generator, Translator
except ImportError:
  Translator = Generator = Any

Mo = TypeVar('Mo')
To = TypeVar('To')

__all_ = ['Runner', 'runner']

class Runner(Protocol[Mo, To]):
  __doc__: str = ...
  __module__: str = ...
  llm: LLM[Mo, To] = ...
  llm_config: LLMConfig = ...
  model: Mo = ...
  tokenizer: To = ...
  llm_type: str = ...
  identifying_params: Dict[str, Any] = ...
  llm_tag: bentoml.Tag = ...
  backend: LiteralBackend = ...
  has_adapters: bool = ...
  template: str = ...
  system_message: str = ...

  @bentoml.api
  async def generate_iterator(
    self, prompt_token_ids: List[int], request_id: str, stop: Optional[Iterable[str]] = ..., adapter_name: Optional[str] = ..., **attrs: Any
  ) -> AsyncGenerator[GenerationOutput, None]: ...

def runner(llm: LLM[M, T], /, **attrs: Unpack[ServiceConfig]) -> Service[Runner[M, T]]: ...
