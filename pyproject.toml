[build-system]
build-backend = "hatchling.build"
requires = [
  "hatchling==1.18.0",
  "hatch-vcs==0.3.0",
  "hatch-fancy-pypi-readme==23.1.0",
]
[project]
authors = [
  { name = "Aaron Pham", email = "aarnphm@bentoml.com" },
  { name = "BentoML Team", email = "contact@bentoml.com" },
]
classifiers = [
  "Development Status :: 5 - Production/Stable",
  "Environment :: GPU :: NVIDIA CUDA",
  "Environment :: GPU :: NVIDIA CUDA :: 12",
  "Environment :: GPU :: NVIDIA CUDA :: 11.8",
  "Environment :: GPU :: NVIDIA CUDA :: 11.7",
  "License :: OSI Approved :: Apache Software License",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries",
  "Operating System :: OS Independent",
  "Intended Audience :: Developers",
  "Intended Audience :: Science/Research",
  "Intended Audience :: System Administrators",
  "Typing :: Typed",
  "Programming Language :: Python",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3 :: Only",
  "Programming Language :: Python :: 3.8",
  "Programming Language :: Python :: 3.9",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: Implementation :: CPython",
  "Programming Language :: Python :: Implementation :: PyPy",
]
description = "OpenLLM monorepo"
dynamic = ["version", "readme", "dependencies"]
keywords = [
  "MLOps",
  "AI",
  "BentoML",
  "Model Serving",
  "Model Deployment",
  "LLMOps",
  "Falcon",
  "Vicuna",
  "Llama 2",
  "Fine tuning",
  "Serverless",
  "Large Language Model",
  "Generative AI",
  "StableLM",
  "Alpaca",
  "PyTorch",
  "Transformers",
]
license = "Apache-2.0"
name = "openllm-monorepo"
requires-python = ">=3.8"
[project.urls]
Blog = "https://modelserving.com"
Chat = "https://discord.gg/openllm"
Documentation = "https://github.com/bentoml/openllm#readme"
GitHub = "https://github.com/bentoml/OpenLLM"
History = "https://github.com/bentoml/OpenLLM/blob/main/CHANGELOG.md"
Homepage = "https://bentoml.com"
Tracker = "https://github.com/bentoml/OpenLLM/issues"
Twitter = "https://twitter.com/bentomlai"
[tool.hatch.version]
fallback-version = "0.0.0"
source = "vcs"
[tool.hatch.version.raw-options]
git_describe_command = [
  "git",
  "describe",
  "--dirty",
  "--tags",
  "--long",
  "--first-parent",
]
local_scheme = "no-local-version"
[tool.hatch.metadata]
allow-direct-references = true

[tool.cibuildwheel]
build-verbosity = 1
# So the following environment will be targeted for compiled wheels:
# - Python: CPython 3.8-3.11
# - Architecture (64-bit only): amd64 / x86_64, universal2, and arm64
# - OS: Linux (no musl), and macOS
build = "cp3*-*"
skip = ["*-manylinux_i686", "*-musllinux_*", "*-win32", "pp-*", "cp312-*"]

[tool.cibuildwheel.environment]
HATCH_BUILD_HOOKS_ENABLE = "1"
MYPYC_DEBUG_LEVEL = "1"
MYPYC_OPT_LEVEL = "3"

[tool.cibuildwheel.linux]
before-build = ["yum install -y clang gcc"]

[tool.cibuildwheel.macos]
before-build = ["rustup target add aarch64-apple-darwin"]

[tool.cibuildwheel.linux.environment]
HATCH_BUILD_HOOKS_ENABLE = "1"
MYPYC_DEBUG_LEVEL = "1"
MYPYC_OPT_LEVEL = "3"
# Use clang to compile successfully on Linux.
CC = "clang"

[tool.towncrier]
directory = "changelog.d"
filename = "CHANGELOG.md"
issue_format = "[#{issue}](https://github.com/bentoml/openllm/issues/{issue})"
name = "openllm"
start_string = "<!-- towncrier release notes start -->\n"
template = "changelog.d/template.md.jinja"
title_format = ""
underlines = ["", "", ""]
[[tool.towncrier.section]]
path = ""
[[tool.towncrier.type]]
directory = "breaking"
name = "Backwards-incompatible Changes"
showcontent = true
[[tool.towncrier.type]]
directory = "deprecation"
name = "Deprecations"
showcontent = true
[[tool.towncrier.type]]
directory = "change"
name = "Changes"
showcontent = true
[[tool.towncrier.type]]
directory = "refactor"
name = "Refactor"
showcontent = true
[[tool.towncrier.type]]
directory = "feature"
name = "Features"
showcontent = true
[[tool.towncrier.type]]
directory = "fix"
name = "Bug fix"
showcontent = true

[tool.interrogate]
fail-under = 100
verbose = 2
whitelist-regex = ["test_.*"]

[tool.check-wheel-contents]
toplevel = ["openllm"]

[tool.ruff]
indent-width = 2
extend-exclude = [
  "tools",
  "examples",
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/models/__init__.py",
  "openllm-client/src/openllm_client/pb/**",
]
extend-include = ["*.ipynb"]
extend-select = [
  "I",    # isort
  "G",    # flake8-logging-format
  "W",    # pycodestyle
  "Q",    # flake8-quotes
  "FA",   # flake8-future-annotations
  "TCH",  # flake8-type-checking
  "PLW",  # pylint-warning
  "PLR",  # pylint-refactor
  "PT",   # flake8-pytest-style
  "PERF", # perflint
  "FLY",  # flynt
  "RUF",  # Ruff-specific rules
  "YTT",  # flake8-2020
]
fix = true
ignore = [
  "PLR0911",
  "PLR0912",
  "PLR0913",
  "PLR0915",
  "PLR2004", # magic value to use constant
  "E501",    # ignore line length violation
  "E401",    # ignore multiple line import
  "E702",
  "TCH004",  # don't move runtime import out, just warn about it
  "RUF012",  # mutable attributes to be used with ClassVar
  "E701",    # multiple statement on single line
]
line-length = 192
target-version = "py312"
typing-modules = ["openllm_core._typing_compat"]
unfixable = ["TCH004"]
[tool.ruff.flake8-type-checking]
exempt-modules = ["typing", "typing_extensions", "openllm_core._typing_compat"]
runtime-evaluated-base-classes = [
  "pydantic.BaseModel",
  "openllm_core._configuration.LLMConfig",
  "openllm_core._configuration.GenerationConfig",
  "openllm_core._configuration.ModelSettings",
  "openllm.LLMConfig",
]
runtime-evaluated-decorators = ["attrs.define", "attrs.frozen", "trait"]
[tool.ruff.format]
quote-style = "single"
indent-style = "space"
skip-magic-trailing-comma = true
[tool.ruff.pydocstyle]
convention = "google"
[tool.ruff.pycodestyle]
ignore-overlong-task-comments = true
[tool.ruff.isort]
combine-as-imports = true
force-single-line = true
force-wrap-aliases = false
known-first-party = ["openllm", "bentoml", "openllm_core", "openllm_client"]
known-third-party = [
  "transformers",
  "click",
  "huggingface_hub",
  "torch",
  "vllm",
  "auto_gptq",
  "peft",
  "click_option_group",
]
lines-after-imports = 1
lines-between-types = 1
no-lines-before = ["future", "standard-library"]
relative-imports-order = "closest-to-furthest"
[tool.ruff.flake8-quotes]
avoid-escape = false
multiline-quotes = "single"
inline-quotes = "single"
docstring-quotes = "single"
[tool.ruff.extend-per-file-ignores]
"openllm-python/src/openllm/_service.py" = ["E401"]
"openllm-python/src/openllm/models/**" = ["E", "F", "I001"]
"openllm-python/tests/**/*" = ["S101", "TID252", "PT011", "S307"]
"openllm-python/src/openllm/_llm.py" = ["F811"]
"openllm-core/src/openllm_core/utils/import_utils.py" = ["PLW0603", "F811"]
"openllm-core/src/openllm_core/_configuration.py" = ["F811"]
"openllm-client/src/openllm_client/bentoclient/_http.py" = ["PERF203"]
"typings/**" = ["F", "E"]

[tool.yapfignore]
ignore_patterns = [
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/models/__init__.py",
  "openllm-client/src/openllm_client/pb/**",
  "examples/openai_client.py"
]

[tool.yapf]
based_on_style = "google"
INDENT_WIDTH = 2
JOIN_MULTIPLE_LINES = true
COLUMN_LIMIT = 192
USE_TABS = false
BLANK_LINES_AROUND_TOP_LEVEL_DEFINITION = 1
BLANK_LINES_BETWEEN_TOP_LEVEL_IMPORTS_AND_VARIABLES = 1
DISABLE_ENDING_COMMA_HEURISTIC = true
BLANK_LINE_BEFORE_CLASS_DOCSTRING = false
BLANK_LINE_BEFORE_MODULE_DOCSTRING = false
BLANK_LINE_BEFORE_NESTED_CLASS_OR_DEF = false
ALIGN_CLOSING_BRACKET_WITH_VISUAL_INDENT = true
ALLOW_MULTILINE_DICTIONARY_KEYS = false
ALLOW_SPLIT_BEFORE_DICT_VALUE = false
COALESCE_BRACKETS = true

[tool.pytest.ini_options]
addopts = ["-rfEX", "-pno:warnings", "--snapshot-warn-unused"]
python_files = ["test_*.py", "*_test.py"]
testpaths = ["openllm-python/tests"]

[tool.coverage.paths]
openllm = [
  "openllm-python/src/openllm",
  "*/openllm-python/src/openllm",
  "openllm-client/src/openllm_client",
  "*/openllm-client/src/openllm_client",
  "openllm-core/src/openllm_core",
  "*/openllm-core/src/openllm_core",
]
[tool.coverage.run]
branch = true
omit = [
  "__pypackages__/*",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
  "openllm-client/src/openllm_client/pb/**",
]
source_pkgs = ["openllm", "openllm_core", "openllm_client"]
[tool.coverage.report]
exclude_lines = [
  "no cov",
  "pragma: no cover",
  "if __name__ == .__main__.:",
  "if t.TYPE_CHECKING:",
  "if _t.TYPE_CHECKING:",
  'if TYPE_CHECKING:',
  'if typing.TYPE_CHECKING:',
  'if t.TYPE_CHECKING and not MYPY:',
  'if DEBUG:',
  'if utils.DEBUG',
  'if openllm.utils.DEBUG',
  'if openllm_core.utils.DEBUG',
  '@_overload',
  '@overload',
  '@t.overload',
  '@typing.overload',
  'raise NotImplementedError',
  'raise NotImplemented',
  'except MissingDependencyError:',
]
omit = [
  "__pypackages__/*",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
  "openllm-client/src/openllm_client/pb/**",
]
precision = 2
show_missing = true

[tool.pyright]
useLibraryCodeForTypes = true
verboseOutput = true
include = ["openllm-core/src", "openllm-client/src", "openllm-python/src"]
defineConstant = { MYPY = true }
exclude = [
  "__pypackages__/*",
  "cz.py",
  "tools",
  "examples",
  "typings",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/models/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-python/tests",
  "openllm-core/src/openllm_core/_typing_compat.py",
  "openllm-client/src/openllm_client/pb/**",
]
pythonVersion = "3.12"
enableExperimentalFeatures = true
reportMissingImports = "warning"
reportMissingTypeStubs = false
reportPrivateUsage = "warning"
reportUnknownArgumentType = "warning"
reportUnknownMemberType = "warning"
reportUnknownVariableType = "warning"
reportUnsupportedDunderAll = "warning"
reportWildcardImportFromLibrary = "warning"
typeCheckingMode = "strict"

[tool.mypy]
exclude = [
  "__pypackages__/*",
  "examples",
  "tools",
  "cz.py",
  "setup.py",
  "openllm-python/setup.py",
  "openllm-client/setup.py",
  "openllm-core/setup.py",
  "openllm-python/tests",
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/models",
  "openllm-python/src/openllm/_service.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
  "openllm-client/src/openllm_client/pb/**",
]
modules = ["openllm", "openllm_core", "openllm_client"]
mypy_path = "typings:openllm-core/src:openllm-client/src"
pretty = true
python_version = "3.8"
show_error_codes = true
strict = true
warn_return_any = false
warn_unreachable = true
warn_unused_ignores = false
explicit_package_bases = true
[[tool.mypy.overrides]]
ignore_missing_imports = true
module = [
  "IPython.*",
  "numpy.*",
  "tensorflow.*",
  "torch.*",
  "optimum.*",
  "inflection.*",
  "huggingface_hub.*",
  "click_option_group.*",
  "peft.*",
  "auto_gptq.*",
  "vllm.*",
  "orjson.*",
  "httpx.*",
  "cloudpickle.*",
  "circus.*",
  "grpc_health.v1.*",
  "transformers.*",
  "ghapi.*",
]
[[tool.mypy.overrides]]
ignore_errors = true
module = [
  "openllm.models.*",
  "openllm.playground.*",
  "openllm_core._typing_compat",
]
