[build-system]
build-backend = "hatchling.build"
requires = [
  "hatchling==1.18.0",
  "hatch-vcs==0.3.0",
  "hatch-fancy-pypi-readme==23.1.0",
]
[project]
authors = [
  { name = "Aaron Pham", email = "aarnphm@bentoml.com" },
  { name = "BentoML Team", email = "contact@bentoml.com" },
]
classifiers = [
  "Development Status :: 5 - Production/Stable",
  "Environment :: GPU :: NVIDIA CUDA",
  "Environment :: GPU :: NVIDIA CUDA :: 12",
  "Environment :: GPU :: NVIDIA CUDA :: 11.8",
  "Environment :: GPU :: NVIDIA CUDA :: 11.7",
  "License :: OSI Approved :: Apache Software License",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries",
  "Operating System :: OS Independent",
  "Intended Audience :: Developers",
  "Intended Audience :: Science/Research",
  "Intended Audience :: System Administrators",
  "Typing :: Typed",
  "Programming Language :: Python",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3 :: Only",
  "Programming Language :: Python :: 3.8",
  "Programming Language :: Python :: 3.9",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: Implementation :: CPython",
  "Programming Language :: Python :: Implementation :: PyPy",
]
description = "OpenLLM monorepo"
dynamic = ["version", "readme", "dependencies"]
keywords = [
  "MLOps",
  "AI",
  "BentoML",
  "Model Serving",
  "Model Deployment",
  "LLMOps",
  "Falcon",
  "Vicuna",
  "Llama 2",
  "Fine tuning",
  "Serverless",
  "Large Language Model",
  "Generative AI",
  "StableLM",
  "Alpaca",
  "PyTorch",
  "Transformers",
]
license = "Apache-2.0"
name = "openllm-monorepo"
requires-python = ">=3.8"
[project.urls]
Blog = "https://modelserving.com"
Chat = "https://discord.gg/openllm"
Documentation = "https://github.com/bentoml/openllm#readme"
GitHub = "https://github.com/bentoml/OpenLLM"
History = "https://github.com/bentoml/OpenLLM/blob/main/CHANGELOG.md"
Homepage = "https://bentoml.com"
Tracker = "https://github.com/bentoml/OpenLLM/issues"
Twitter = "https://twitter.com/bentomlai"
[tool.hatch.version]
fallback-version = "0.0.0"
source = "vcs"
[tool.hatch.version.raw-options]
git_describe_command = [
  "git",
  "describe",
  "--dirty",
  "--tags",
  "--long",
  "--first-parent",
]
local_scheme = "no-local-version"
[tool.hatch.metadata]
allow-direct-references = true

[tool.cibuildwheel]
build-verbosity = 1
# So the following environment will be targeted for compiled wheels:
# - Python: CPython 3.8-3.11
# - Architecture (64-bit only): amd64 / x86_64, universal2, and arm64
# - OS: Linux (no musl), and macOS
build = "cp3*-*"
skip = ["*-manylinux_i686", "*-musllinux_*", "*-win32", "pp-*", "cp312-*"]

[tool.cibuildwheel.environment]
HATCH_BUILD_HOOKS_ENABLE = "1"
MYPYC_DEBUG_LEVEL = "1"
MYPYC_OPT_LEVEL = "3"

[tool.cibuildwheel.linux]
before-build = ["yum install -y clang gcc"]

[tool.cibuildwheel.macos]
before-build = ["rustup target add aarch64-apple-darwin"]

[tool.cibuildwheel.linux.environment]
HATCH_BUILD_HOOKS_ENABLE = "1"
MYPYC_DEBUG_LEVEL = "1"
MYPYC_OPT_LEVEL = "3"
# Use clang to compile successfully on Linux.
CC = "clang"

[tool.towncrier]
directory = "changelog.d"
filename = "CHANGELOG.md"
issue_format = "[#{issue}](https://github.com/bentoml/openllm/issues/{issue})"
name = "openllm"
start_string = "<!-- towncrier release notes start -->\n"
template = "changelog.d/template.md.jinja"
title_format = ""
underlines = ["", "", ""]
[[tool.towncrier.section]]
path = ""
[[tool.towncrier.type]]
directory = "breaking"
name = "Backwards-incompatible Changes"
showcontent = true
[[tool.towncrier.type]]
directory = "deprecation"
name = "Deprecations"
showcontent = true
[[tool.towncrier.type]]
directory = "change"
name = "Changes"
showcontent = true
[[tool.towncrier.type]]
directory = "refactor"
name = "Refactor"
showcontent = true
[[tool.towncrier.type]]
directory = "feature"
name = "Features"
showcontent = true
[[tool.towncrier.type]]
directory = "fix"
name = "Bug fix"
showcontent = true

[tool.interrogate]
fail-under = 100
verbose = 2
whitelist-regex = ["test_.*"]

[tool.check-wheel-contents]
toplevel = ["openllm"]

[tool.ruff]
extend-exclude = [
  "tools",
  "examples",
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-python/src/openllm/models/__init__.py",
]
extend-include = ["*.ipynb"]
extend-select = [
  "I",    # isort
  "G",    # flake8-logging-format
  "W",    # pycodestyle
  "Q",    # flake8-quotes
  "FA",   # flake8-future-annotations
  "TCH",  # flake8-type-checking
  "PLW",  # pylint-warning
  "PLR",  # pylint-refactor
  "PT",   # flake8-pytest-style
  "PERF", # perflint
  "FLY",  # flynt
  "RUF",  # Ruff-specific rules
  "YTT",  # flake8-2020
]
fix = true
ignore = [
  "FBT003",  # Allow boolean positional values in function calls, like `dict.get(... True)`
  "PLR0911",
  "PLR0912",
  "PLR0913",
  "PLR0915",
  "PLR2004", # magic value to use constant
  "E501",    # ignore line length violation
  "E401",    # ignore multiple line import
  "E702",
  "I001",    # unsorted imports
  "TCH004",  # don't move runtime import out, just warn about it
  "RUF012",  # mutable attributes to be used with ClassVar
  "B905",    # zip warning about strict, only applicable for 3.10+
  "E701",    # multiple statement on single line
]
line-length = 768
target-version = "py312"
typing-modules = ["openllm_core._typing_compat"]
unfixable = ["TCH004"]
[tool.ruff.flake8-type-checking]
exempt-modules = ["typing", "typing_extensions", "openllm_core._typing_compat"]
runtime-evaluated-base-classes = [
  "pydantic.BaseModel",
  "openllm_core._configuration.LLMConfig",
  "openllm_core._configuration.GenerationConfig",
  "openllm_core._configuration.ModelSettings",
  "openllm.LLMConfig",
]
runtime-evaluated-decorators = ["attrs.define", "attrs.frozen", "trait"]
[tool.ruff.pydocstyle]
convention = "google"
[tool.ruff.pycodestyle]
ignore-overlong-task-comments = true
[tool.ruff.isort]
combine-as-imports = true
force-single-line = false
force-wrap-aliases = true
known-first-party = ["openllm", "bentoml", "openllm_core", "openllm_client"]
known-third-party = [
  "transformers",
  "click",
  "huggingface_hub",
  "torch",
  "vllm",
  "auto_gptq",
]
lines-after-imports = 0
lines-between-types = 0
no-lines-before = ["future", "standard-library"]
relative-imports-order = "closest-to-furthest"
required-imports = ["from __future__ import annotations"]
[tool.ruff.flake8-quotes]
avoid-escape = false
multiline-quotes = "single"
inline-quotes = "single"
docstring-quotes = "single"
[tool.ruff.extend-per-file-ignores]
"openllm-python/src/openllm/_service.py" = ["E401"]
"openllm-python/src/openllm/models/**" = ["E", "F"]
"openllm-python/tests/**/*" = ["S101", "TID252", "PT011", "S307"]
"openllm-python/src/openllm/_llm.py" = ["F811"]
"openllm-core/src/openllm_core/utils/import_utils.py" = ["PLW0603", "F811"]
"openllm-core/src/openllm_core/_configuration.py" = ["F811"]
"openllm-client/src/openllm_client/bentoclient/_http.py" = ["PERF203"]
"typings/**" = ["F", "E"]

[tool.yapfignore]
ignore_patterns = [
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-python/src/openllm/models/__init__.py",
]

[tool.yapf]
ALIGN_CLOSING_BRACKET_WITH_VISUAL_INDENT = true
ALLOW_MULTILINE_DICTIONARY_KEYS = false
ALLOW_MULTILINE_LAMBDAS = false
ALLOW_SPLIT_BEFORE_DEFAULT_OR_NAMED_ASSIGNS = false
ALLOW_SPLIT_BEFORE_DICT_VALUE = false
ARITHMETIC_PRECEDENCE_INDICATION = true
BLANK_LINES_AROUND_TOP_LEVEL_DEFINITION = 0
BLANK_LINES_BETWEEN_TOP_LEVEL_IMPORTS_AND_VARIABLES = 0
BLANK_LINE_BEFORE_CLASS_DOCSTRING = false
BLANK_LINE_BEFORE_MODULE_DOCSTRING = false
BLANK_LINE_BEFORE_NESTED_CLASS_OR_DEF = false
COALESCE_BRACKETS = true
COLUMN_LIMIT = 192
CONTINUATION_ALIGN_STYLE = "VALIGN-RIGHT"
DEDENT_CLOSING_BRACKETS = true
DISABLE_ENDING_COMMA_HEURISTIC = true
EACH_DICT_ENTRY_ON_SEPARATE_LINE = true
INDENT_BLANK_LINES = false
INDENT_CLOSING_BRACKETS = false
INDENT_WIDTH = 2
JOIN_MULTIPLE_LINES = true
NO_SPACES_AROUND_SELECTED_BINARY_OPERATORS = true
SPACES_AROUND_SUBSCRIPT_COLON = false
SPACES_AROUND_DICT_DELIMITERS = false
SPACES_AROUND_LIST_DELIMITERS = false
SPACES_AROUND_POWER_OPERATOR = false
SPACES_AROUND_TUPLE_DELIMITERS = false
SPACE_BETWEEN_ENDING_COMMA_AND_CLOSING_BRACKET = false
SPACE_INSIDE_BRACKETS = false
SPLIT_ALL_COMMA_SEPARATED_VALUES = false
SPLIT_ALL_TOP_LEVEL_COMMA_SEPARATED_VALUES = true
SPLIT_ARGUMENTS_WHEN_COMMA_TERMINATED = false
SPLIT_BEFORE_BITWISE_OPERATOR = false
SPLIT_BEFORE_CLOSING_BRACKET = false
SPLIT_BEFORE_DICT_SET_GENERATOR = false
SPLIT_BEFORE_DOT = true # similar to how rust format its expression
SPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN = false
SPLIT_BEFORE_FIRST_ARGUMENT = false
SPLIT_BEFORE_LOGICAL_OPERATOR = false
SPLIT_BEFORE_NAMED_ASSIGNS = false
SPLIT_COMPLEX_COMPREHENSION = true
SPLIT_PENALTY_AFTER_OPENING_BRACKET = 350
SPLIT_PENALTY_BEFORE_IF_EXPR = 10000
SPLIT_PENALTY_COMPREHENSION = 2500
SPLIT_PENALTY_FOR_ADDED_LINE_SPLIT = 5000
USE_TABS = false

[tool.pytest.ini_options]
addopts = ["-rfEX", "-pno:warnings", "--snapshot-warn-unused"]
python_files = ["test_*.py", "*_test.py"]
testpaths = ["openllm-python/tests"]

[tool.coverage.paths]
openllm = ["openllm-python/src/openllm", "*/openllm-python/src/openllm"]
[tool.coverage.run]
branch = true
omit = [
  "__pypackages__/*",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
]
source_pkgs = ["openllm"]
[tool.coverage.report]
exclude_lines = [
  "no cov",
  "pragma: no cover",
  "if __name__ == .__main__.:",
  "if t.TYPE_CHECKING:",
  "if _t.TYPE_CHECKING:",
  'if TYPE_CHECKING:',
  'if typing.TYPE_CHECKING:',
  'if t.TYPE_CHECKING and not MYPY:',
  'if DEBUG:',
  'if utils.DEBUG',
  'if openllm.utils.DEBUG',
  'if openllm_core.utils.DEBUG',
  '@_overload',
  '@overload',
  '@t.overload',
  '@typing.overload',
  'raise NotImplementedError',
  'raise NotImplemented',
  'except MissingDependencyError:',
]
omit = [
  "__pypackages__/*",
  "openllm-python/src/openllm/_version.py",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
]
precision = 2
show_missing = true

# NOTE: The following strict type checker we are using:
# - mypy: for both runtime and static type checking (since we compile the wheels )
# - pyright: for static type checking only, as we want to use newer type feature
[tool.pyright]
analysis.useLibraryCodeForTypes = true
exclude = [
  "__pypackages__/*",
  "openllm-python/src/openllm/playground/",
  "openllm-python/src/openllm/models/",
  "openllm-python/src/openllm/__init__.py",
  "openllm-python/src/openllm/__main__.py",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-python/tests",
  "openllm-core/src/openllm_core/_typing_compat.py",
  "cz.py",
  "hatch_build.py",
  "tools",
  "examples",
  "typings",
]
executionEnvironments = [
  { root = "openllm-python/src" },
  { root = "openllm-core/src" },
  { root = "openllm-client/src" },
]
pythonVersion = "3.12"
enableExperimentalFeatures = true
reportMissingImports = "warning"
reportMissingTypeStubs = false
reportPrivateUsage = "warning"
reportUnknownArgumentType = "warning"
reportUnknownMemberType = "warning"
reportUnknownVariableType = "warning"
reportUnsupportedDunderAll = "warning"
reportWildcardImportFromLibrary = "warning"
typeCheckingMode = "strict"

[tool.mypy]
exclude = [
  "examples",
  "tools",
  "cz.py",
  "openllm-python/tests",
  "openllm-python/src/openllm/playground",
  "openllm-python/src/openllm/utils/dummy_*.py",
  "openllm-python/src/openllm/models",
  "openllm-python/src/openllm/_service.py",
  "openllm-core/src/openllm_core/_typing_compat.py",
]
modules = ["openllm", "openllm_core", "openllm_client"]
mypy_path = "typings:openllm-core/src:openllm-client/src"
pretty = true
python_version = "3.8"
show_error_codes = true
strict = true
warn_return_any = false
warn_unreachable = true
warn_unused_ignores = false
explicit_package_bases = true
[[tool.mypy.overrides]]
ignore_missing_imports = true
module = [
  "IPython.*",
  "numpy.*",
  "tensorflow.*",
  "torch.*",
  "optimum.*",
  "inflection.*",
  "huggingface_hub.*",
  "click_option_group.*",
  "peft.*",
  "auto_gptq.*",
  "vllm.*",
  "orjson.*",
  "httpx.*",
  "cloudpickle.*",
  "circus.*",
  "grpc_health.v1.*",
  "transformers.*",
  "ghapi.*",
]
[[tool.mypy.overrides]]
ignore_errors = true
module = [
  "openllm.models.*",
  "openllm.playground.*",
  "openllm_core._typing_compat",
]
